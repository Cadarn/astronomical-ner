{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607d5e71",
   "metadata": {},
   "source": [
    "# Data Annotation for Astronomical NER\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this notebook is to process raw NASA ADS abstract data into a structured format suitable for training a spaCy Named Entity Recognition (NER) model. This involves identifying astronomical source names (like \"SN 2023ixf\" or \"Crab Nebula\") in the text and marking their exact locations.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The process consists of the following key steps:\n",
    "\n",
    "1.  **Load Raw Data**: Ingest the downloaded JSON files containing abstracts and metadata from NASA ADS.\n",
    "2.  **Extract Gold-Standard Entities**: Use the `keywords` field in the data to identify \"gold-standard\" astronomical entities. We specifically look for keywords prefixed with `individual:`, which reliably tag specific celestial objects.\n",
    "3.  **Locate Entities in Text**: Search for these extracted entities within the corresponding document's `title` and `abstract`.\n",
    "4.  **Format for spaCy**: Structure the text and entity locations into the specific format required by spaCy for training, which is a list of tuples, where each tuple contains the text and a dictionary of entity spans.\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "-   `extract_astronomical_keyword_entities()`: Parses the `keyword` list from a document to pull out any terms identified as an \"individual\" celestial object.\n",
    "-   `find_exact_matches()`: A robust function that uses regular expressions with word boundaries (`\\b`) to find the precise start and end character offsets of an entity string in a body of text. This prevents partial matches (e.g., finding \"Norma\" inside \"34 Normae\").\n",
    "-   `build_data_model()`: The main orchestration function that takes a raw document, runs the extraction and search steps, and compiles the final structured output.\n",
    "-   `build_spacy_ner_data()`: Formats the final text and entity locations into the `(text, {\"entities\": [...]})` tuple structure that spaCy expects for a single training example.\n",
    "\n",
    "## Final Output\n",
    "\n",
    "The primary output of this notebook is a list of spaCy training examples stored in the `spacy_ner_data` key of the generated data model. This data can be saved and used directly in a spaCy training pipeline to teach the model to recognize astronomical objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d92efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b70c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_astronomical_keyword_entities(doc: dict) -> list[str]:\n",
    "    \"\"\"Extracts astronomical entities from document keywords.\n",
    "\n",
    "    This function iterates through the keywords of a NASA ADS document record\n",
    "    and extracts entities specifically marked with the \"individual: \" prefix.\n",
    "\n",
    "    Args:\n",
    "        doc: A dictionary representing a single document from the ADS API.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique astronomical entity names.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    keywords = doc.get(\"keyword\", [])\n",
    "    for keyword in keywords:\n",
    "        if \"individual: \" in keyword.lower():\n",
    "            entity = keyword.split(\"individual: \")[-1].strip()\n",
    "            entities.append(entity)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def remove_overlapping_entities(entities: list[str]) -> list[str]:\n",
    "    \"\"\"Remove entities that are substrings of other entities.\n",
    "    \n",
    "    This prevents overlapping entity matches like 'HD 189733' and 'HD 189733 b'\n",
    "    by keeping only the longest version of each entity.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity strings\n",
    "        \n",
    "    Returns:\n",
    "        List of entities with overlapping substrings removed\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return entities\n",
    "    \n",
    "    # Sort by length (longest first) to prioritize longer matches\n",
    "    sorted_entities = sorted(entities, key=len, reverse=True)\n",
    "    filtered_entities = []\n",
    "    \n",
    "    for entity in sorted_entities:\n",
    "        # Check if this entity is a substring of any already accepted entity\n",
    "        is_substring = False\n",
    "        for accepted_entity in filtered_entities:\n",
    "            if entity != accepted_entity and entity in accepted_entity:\n",
    "                is_substring = True\n",
    "                break\n",
    "        \n",
    "        if not is_substring:\n",
    "            filtered_entities.append(entity)\n",
    "    \n",
    "    return filtered_entities\n",
    "\n",
    "\n",
    "def find_entities_in_text(\n",
    "    text: str, entities: list[str]\n",
    ") -> dict[str, list[tuple[int, int]]]:\n",
    "    \"\"\"Finds all occurrences of a list of entity strings in a text.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search within.\n",
    "        entities: A list of string entities to find.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are the found entities and values are lists\n",
    "        of (start, end) character offset tuples for each occurrence.\n",
    "    \"\"\"\n",
    "    found_entities = {}\n",
    "    for entity in entities:\n",
    "        if locations := find_exact_matches(text, entity):\n",
    "            found_entities[entity] = locations\n",
    "    return found_entities\n",
    "\n",
    "\n",
    "def find_exact_matches(text: str, search_string: str) -> list[tuple[int, int]]:\n",
    "    \"\"\"Finds occurrences of the exact string using word boundaries.\n",
    "\n",
    "    This ensures that substrings like \"34 Normae\" are not matched when searching\n",
    "    for \"Normae\".\n",
    "\n",
    "    Args:\n",
    "        text: The string to search within.\n",
    "        search_string: The exact string to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the (start, end) index\n",
    "        of a match. The end index is inclusive.\n",
    "    \"\"\"\n",
    "    if not search_string:\n",
    "        return []\n",
    "\n",
    "    # Use word boundaries to ensure whole-word matching.\n",
    "    pattern = re.compile(r'\\b' + re.escape(search_string) + r'\\b')\n",
    "\n",
    "    occurrences = []\n",
    "    for match in pattern.finditer(text):\n",
    "        start_index = match.start()\n",
    "        end_index = match.end() - 1  # End index is inclusive\n",
    "        occurrences.append((start_index, end_index))\n",
    "    return occurrences\n",
    "\n",
    "\n",
    "def resolve_overlapping_spans(entities_dict: dict[str, list[tuple[int, int]]]) -> list[tuple[int, int, str]]:\n",
    "    \"\"\"Resolve overlapping entity spans by keeping the longest match.\n",
    "    \n",
    "    When multiple entities overlap in the text, keep only the longest one.\n",
    "    This prevents spaCy training errors from conflicting entity spans.\n",
    "    \n",
    "    Args:\n",
    "        entities_dict: Dictionary mapping entity names to their span locations\n",
    "        \n",
    "    Returns:\n",
    "        List of (start, end, label) tuples with overlaps resolved\n",
    "    \"\"\"\n",
    "    # Flatten all entities into (start, end, entity_name) tuples\n",
    "    all_spans = []\n",
    "    for entity_name, locations in entities_dict.items():\n",
    "        for start, end in locations:\n",
    "            all_spans.append((start, end, entity_name))\n",
    "    \n",
    "    # Sort by start position, then by span length (longest first)\n",
    "    all_spans.sort(key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    \n",
    "    # Remove overlapping spans, keeping longest ones\n",
    "    resolved_spans = []\n",
    "    for start, end, entity_name in all_spans:\n",
    "        # Check if this span overlaps with any already accepted span\n",
    "        overlaps = False\n",
    "        for accepted_start, accepted_end, _ in resolved_spans:\n",
    "            # Check for overlap: spans overlap if one starts before the other ends\n",
    "            if not (end < accepted_start or start > accepted_end):\n",
    "                overlaps = True\n",
    "                break\n",
    "        \n",
    "        if not overlaps:\n",
    "            resolved_spans.append((start, end, \"ASTRO_OBJ\"))\n",
    "    \n",
    "    return resolved_spans\n",
    "\n",
    "\n",
    "def search_title_for_entities(\n",
    "    doc: dict, entities: list[str]\n",
    ") -> tuple[str, dict[str, list[tuple[int, int]]]]:\n",
    "    \"\"\"Searches the document title for specified entities.\n",
    "\n",
    "    Args:\n",
    "        doc: A dictionary representing a single document from the ADS API.\n",
    "        entities: A list of string entities to find.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the document title text and a dictionary of\n",
    "        found entities with their character offsets.\n",
    "    \"\"\"\n",
    "    text = \" \".join(doc.get(\"title\", []))\n",
    "    found_entities = find_entities_in_text(text, entities)\n",
    "    return text, found_entities\n",
    "\n",
    "\n",
    "def search_abstract_for_entities(\n",
    "    doc: dict, entities: list[str]\n",
    ") -> tuple[str, dict[str, list[tuple[int, int]]]]:\n",
    "    \"\"\"Searches the document abstract for specified entities.\n",
    "\n",
    "    Args:\n",
    "        doc: A dictionary representing a single document from the ADS API.\n",
    "        entities: A list of string entities to find.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the document abstract text and a dictionary of\n",
    "        found entities with their character offsets.\n",
    "    \"\"\"\n",
    "    text = doc.get(\"abstract\", \"\")\n",
    "    found_entities = find_entities_in_text(text, entities)\n",
    "    return text, found_entities\n",
    "\n",
    "def build_data_model(doc):\n",
    "    \"\"\"\n",
    "    Builds the data structure for training a NER model.\n",
    "    \"\"\"\n",
    "    raw_entities = extract_astronomical_keyword_entities(doc)\n",
    "    if not raw_entities:\n",
    "        return None\n",
    "    \n",
    "    # Remove overlapping entities (e.g., keep \"HD 189733 b\" instead of both \"HD 189733\" and \"HD 189733 b\")\n",
    "    entities = remove_overlapping_entities(raw_entities)\n",
    "    \n",
    "    data_model = {\n",
    "        \"doc_id\": doc.get(\"id\"),\n",
    "        \"title\": doc.get(\"title\", \"\"),\n",
    "        \"abstract\": doc.get(\"abstract\", \"\"),\n",
    "        \"keywords\": doc.get(\"keyword\", []),\n",
    "        \"objects\": entities,\n",
    "        \"raw_objects\": raw_entities,  # Keep original for debugging\n",
    "    }\n",
    "    \n",
    "    title, title_entities = search_title_for_entities(doc, entities)\n",
    "    abstract, abstract_entities = search_abstract_for_entities(doc, entities)\n",
    "\n",
    "    data_model[\"title_entities\"] = title_entities\n",
    "    data_model[\"abstract_entities\"] = abstract_entities\n",
    "\n",
    "    spacy_ner_data = []\n",
    "    if title_entities:\n",
    "        spacy_ner_data.extend(build_spacy_ner_data(title, title_entities))\n",
    "    if abstract_entities:\n",
    "        spacy_ner_data.extend(build_spacy_ner_data(abstract, abstract_entities))\n",
    "    data_model[\"spacy_ner_data\"] = spacy_ner_data\n",
    "    \n",
    "    return data_model\n",
    "\n",
    "def build_spacy_ner_data(text: str, entities: dict[str, list[tuple[int]]]) -> list[dict]:\n",
    "    \"\"\"Builds the data structure for training a spaCy NER model.\n",
    "\n",
    "    This function converts a text and a dictionary of entity locations into the\n",
    "    specific format required for a single spaCy training example. The format is\n",
    "    a list containing the text and a dictionary with an \"entities\" key.\n",
    "    The value of \"entities\" is a list of tuples, where each tuple represents\n",
    "    a single entity with its start offset, end offset (inclusive), and label.\n",
    "\n",
    "    Args:\n",
    "        text: The source text containing the entities.\n",
    "        entities: A dictionary where keys are entity names and values are lists\n",
    "                  of (start, end) character offsets for each occurrence.\n",
    "\n",
    "    Returns:\n",
    "        A list containing the text and an entity dictionary, formatted for\n",
    "        spaCy training. For example:\n",
    "        ['Some text about SN 2023ixf.', {'entities': [(16, 25, 'ASTRO_OBJ')]}]\n",
    "    \"\"\"\n",
    "    # Use the overlap resolution function to clean up conflicting spans\n",
    "    resolved_entities = resolve_overlapping_spans(entities)\n",
    "    \n",
    "    ner_data = [text, {\"entities\": resolved_entities}]\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60cc769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved spaCy NER data to ../data/ner_data/spacy_ner_data.json\n"
     ]
    }
   ],
   "source": [
    "def read_abstracts(data_dir: Path) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reads ADS abstracts from JSON files in the given directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents an abstract.\n",
    "    \"\"\"\n",
    "    abstract_files = list(data_dir.glob(\"*.json\"))\n",
    "    all_abstracts: list[dict[str, Any]] = []\n",
    "    for file in abstract_files:\n",
    "        with open(file, \"r\") as f:\n",
    "            abstract_batch = json.load(f).get(\"response\")\n",
    "            docs = abstract_batch.get(\"docs\", [])\n",
    "            all_abstracts.extend(docs)\n",
    "    return all_abstracts\n",
    "\n",
    "\n",
    "def process_abstracts(abstracts: list[dict[str, Any]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a list of abstracts to build data models.\n",
    "\n",
    "    Args:\n",
    "        abstracts: A list of dictionaries, where each dictionary represents an abstract.\n",
    "\n",
    "    Returns:\n",
    "        A list of data models (dictionaries) built from the abstracts.\n",
    "    \"\"\"\n",
    "    spacy_ner_data: list[dict[str, Any]] = []\n",
    "    for doc in abstracts:\n",
    "        data_model = build_data_model(doc)\n",
    "        if data_model:\n",
    "            spacy_ner_data.append(data_model)\n",
    "    return spacy_ner_data\n",
    "\n",
    "\n",
    "def write_data(data: list[dict[str, Any]], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Writes the processed data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data: A list of dictionaries to write to the file.\n",
    "        output_path: The path to the output JSON file.\n",
    "    \"\"\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved spaCy NER data to {output_path}\")\n",
    "\n",
    "\n",
    "def main(data_root: Path, output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Main function to execute the data processing pipeline.\n",
    "\n",
    "    Args:\n",
    "        data_root: The root directory containing the ADS abstracts.\n",
    "        output_path: The path to save the processed data.\n",
    "    \"\"\"\n",
    "    ads_data = data_root / \"ads_abstracts\"\n",
    "    abstracts = read_abstracts(ads_data)\n",
    "    processed_data = process_abstracts(abstracts)\n",
    "    write_data(processed_data, output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_ROOT = Path(\"../data\")\n",
    "    OUTPATH = DATA_ROOT / \"ner_data\" / \"spacy_ner_data.json\"\n",
    "    main(DATA_ROOT, OUTPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6359314",
   "metadata": {},
   "source": [
    "## Example implementation\n",
    "\n",
    "Testing the functions before we build a more generic script\n",
    "\n",
    "### Test overlap detection and resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8f46e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original entities: ['HD 189733', 'HD 189733 b', 'Fomalhaut', 'Fomalhaut b', 'NGC 4258', 'Crab Nebula']\n",
      "Cleaned entities: ['HD 189733 b', 'Fomalhaut b', 'Crab Nebula', 'NGC 4258']\n",
      "\n",
      "Test text: 'HD 189733 b orbits HD 189733 every 2.2 days'\n",
      "Entity spans before resolution: {'HD 189733': [(0, 8), (17, 25)], 'HD 189733 b': [(0, 10)]}\n",
      "Resolved spans: [(0, 10, 'ASTRO_OBJ'), (17, 25, 'ASTRO_OBJ')]\n"
     ]
    }
   ],
   "source": [
    "# Test overlap detection functions\n",
    "test_entities = [\"HD 189733\", \"HD 189733 b\", \"Fomalhaut\", \"Fomalhaut b\", \"NGC 4258\", \"Crab Nebula\"]\n",
    "print(\"Original entities:\", test_entities)\n",
    "cleaned_entities = remove_overlapping_entities(test_entities)\n",
    "print(\"Cleaned entities:\", cleaned_entities)\n",
    "\n",
    "# Test overlap resolution in spans\n",
    "test_text = \"HD 189733 b orbits HD 189733 every 2.2 days\"\n",
    "test_entities_dict = {\n",
    "    \"HD 189733\": [(0, 8), (17, 25)],\n",
    "    \"HD 189733 b\": [(0, 10)]\n",
    "}\n",
    "print(f\"\\nTest text: '{test_text}'\")\n",
    "print(\"Entity spans before resolution:\", test_entities_dict)\n",
    "resolved_spans = resolve_overlapping_spans(test_entities_dict)\n",
    "print(\"Resolved spans:\", resolved_spans)\n",
    "\n",
    "# Data inputs\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "ADS_DATA = DATA_ROOT / \"ads_abstracts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03639f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 14921651: ['HD 189733 b', 'HD 189733'] -> ['HD 189733 b']\n",
      "Doc 14917098: ['GRB 140506A', 'GRB 140506A host'] -> ['GRB 140506A host']\n",
      "Doc 14916547: ['HD 97658', 'HD 97658 b'] -> ['HD 97658 b']\n",
      "Doc 14946658: ['WASP 52b', 'WASP 52'] -> ['WASP 52b']\n",
      "Doc 14946620: ['SWIFT J1753.5-0127', 'SWIFT J1753.5-0127 - X-rays: binaries'] -> ['SWIFT J1753.5-0127 - X-rays: binaries']\n",
      "Doc 14926052: ['M31', 'M31N 2008-12a'] -> ['M31N 2008-12a']\n",
      "Doc 14949374: ['Cyg X-1', 'Cyg X-1 - X-rays: binaries'] -> ['Cyg X-1 - X-rays: binaries']\n",
      "Doc 14947993: ['4U 1820-30 - X-rays: stars', '4U 1820-30'] -> ['4U 1820-30 - X-rays: stars']\n",
      "Doc 14921476: ['HAT-P-33', 'HAT-P-33b'] -> ['HAT-P-33b']\n",
      "Doc 14946477: ['Fomalhaut b', 'Fomalhaut'] -> ['Fomalhaut b']\n",
      "Doc 14946329: ['382004', '(315898)', '342842', '315898'] -> ['(315898)', '382004', '342842']\n",
      "Doc 14926172: ['M31', 'M31N 2008-12a'] -> ['M31N 2008-12a']\n",
      "Doc 14926048: ['FW Tau', 'FW Tau C'] -> ['FW Tau C']\n",
      "Doc 14921883: ['HD 3167b', 'HD 3167'] -> ['HD 3167b']\n",
      "Doc 14945412: ['Sextans', 'name: Sextans'] -> ['name: Sextans']\n",
      "Doc 14922670: ['GQ Lup', 'GQ Lup B'] -> ['GQ Lup B']\n",
      "Doc 14918209: ['Proxima Cen b', 'Proxima Cen'] -> ['Proxima Cen b']\n",
      "Doc 25451723: ['HAT-P-32A', 'HAT-P-32Ab'] -> ['HAT-P-32Ab']\n",
      "Doc 25420173: ['WASP-46', 'WASP-46b'] -> ['WASP-46b']\n",
      "Doc 14947530: ['CW Leo - stars: late-type', 'CW Leo'] -> ['CW Leo - stars: late-type']\n",
      "Doc 14947700: ['M12', 'NGC 6362', 'NGC 6752', 'M5', 'M55', 'NGC 3201', 'NGC 362', 'M22', 'M4', 'M30', '47 Tucanae', 'M10'] -> ['47 Tucanae', 'NGC 6362', 'NGC 6752', 'NGC 3201', 'NGC 362', 'M12', 'M55', 'M22', 'M30', 'M10', 'M4']\n",
      "Doc 14921843: ['GJ 1132', 'GJ 1132b'] -> ['GJ 1132b']\n",
      "Doc 14921805: ['HD 189733 b', 'HD 189733'] -> ['HD 189733 b']\n",
      "Doc 14921735: ['KELT-12', 'KELT-12b'] -> ['KELT-12b']\n",
      "Doc 14922494: ['NGC 7793 P13', 'NGC 7793'] -> ['NGC 7793 P13']\n",
      "\n",
      "Overlap cleaning statistics:\n",
      "Total documents processed: 28732\n",
      "Documents with overlapping entities: 25\n",
      "Total overlapping entities removed: 25\n"
     ]
    }
   ],
   "source": [
    "# Process ADS abstracts with overlap detection\n",
    "abstract_files = list(ADS_DATA.glob(\"*.json\"))\n",
    "spacy_ner_data = []\n",
    "overlap_stats = {\"total_docs\": 0, \"docs_with_overlaps\": 0, \"entities_removed\": 0}\n",
    "\n",
    "for file in abstract_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        abstract_batch = json.load(f).get(\"response\")\n",
    "        docs = abstract_batch.get(\"docs\", [])\n",
    "    for doc in docs:\n",
    "        overlap_stats[\"total_docs\"] += 1\n",
    "        \n",
    "        # Get raw entities\n",
    "        raw_entities = extract_astronomical_keyword_entities(doc)\n",
    "        if raw_entities:\n",
    "            # Clean overlapping entities\n",
    "            cleaned_entities = remove_overlapping_entities(raw_entities)\n",
    "            \n",
    "            # Track statistics\n",
    "            if len(cleaned_entities) < len(raw_entities):\n",
    "                overlap_stats[\"docs_with_overlaps\"] += 1\n",
    "                overlap_stats[\"entities_removed\"] += len(raw_entities) - len(cleaned_entities)\n",
    "                print(f\"Doc {doc.get('id', 'unknown')}: {raw_entities} -> {cleaned_entities}\")\n",
    "        \n",
    "        data_model = build_data_model(doc)\n",
    "        if data_model:\n",
    "            spacy_ner_data.append(data_model)\n",
    "\n",
    "print(f\"\\nOverlap cleaning statistics:\")\n",
    "print(f\"Total documents processed: {overlap_stats['total_docs']}\")\n",
    "print(f\"Documents with overlapping entities: {overlap_stats['docs_with_overlaps']}\")\n",
    "print(f\"Total overlapping entities removed: {overlap_stats['entities_removed']}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d0cb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002c0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved spaCy NER data to ../data/ner_data/spacy_ner_data.json\n"
     ]
    }
   ],
   "source": [
    "#Output data to disk\n",
    "OUTPATH = DATA_ROOT / \"ner_data\" / \"spacy_ner_data.json\"\n",
    "\n",
    "with open(OUTPATH, \"w\") as f:\n",
    "    json.dump(spacy_ner_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved spaCy NER data to {OUTPATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
