{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607d5e71",
   "metadata": {},
   "source": [
    "# Data Annotation for Astronomical NER\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this notebook is to process raw NASA ADS abstract data into a structured format suitable for training a spaCy Named Entity Recognition (NER) model. This involves identifying astronomical source names (like \"SN 2023ixf\" or \"Crab Nebula\") in the text and marking their exact locations.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The process consists of the following key steps:\n",
    "\n",
    "1.  **Load Raw Data**: Ingest the downloaded JSON files containing abstracts and metadata from NASA ADS.\n",
    "2.  **Extract Gold-Standard Entities**: Use the `keywords` field in the data to identify \"gold-standard\" astronomical entities. We specifically look for keywords prefixed with `individual:`, which reliably tag specific celestial objects.\n",
    "3.  **Locate Entities in Text**: Search for these extracted entities within the corresponding document's `title` and `abstract`.\n",
    "4.  **Format for spaCy**: Structure the text and entity locations into the specific format required by spaCy for training, which is a list of tuples, where each tuple contains the text and a dictionary of entity spans.\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "-   `extract_astronomical_keyword_entities()`: Parses the `keyword` list from a document to pull out any terms identified as an \"individual\" celestial object.\n",
    "-   `find_exact_matches()`: A robust function that uses regular expressions with word boundaries (`\\b`) to find the precise start and end character offsets of an entity string in a body of text. This prevents partial matches (e.g., finding \"Norma\" inside \"34 Normae\").\n",
    "-   `build_data_model()`: The main orchestration function that takes a raw document, runs the extraction and search steps, and compiles the final structured output.\n",
    "-   `build_spacy_ner_data()`: Formats the final text and entity locations into the `(text, {\"entities\": [...]})` tuple structure that spaCy expects for a single training example.\n",
    "\n",
    "## Final Output\n",
    "\n",
    "The primary output of this notebook is a list of spaCy training examples stored in the `spacy_ner_data` key of the generated data model. This data can be saved and used directly in a spaCy training pipeline to teach the model to recognize astronomical objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d92efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    " from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b70c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_astronomical_keyword_entities(doc: dict) -> list[str]:\n",
    "    \"\"\"Extracts astronomical entities from document keywords.\n",
    "\n",
    "    This function iterates through the keywords of a NASA ADS document record\n",
    "    and extracts entities specifically marked with the \"individual: \" prefix.\n",
    "\n",
    "    Args:\n",
    "        doc: A dictionary representing a single document from the ADS API.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique astronomical entity names.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    keywords = doc.get(\"keyword\", [])\n",
    "    for keyword in keywords:\n",
    "        if \"individual: \" in keyword.lower():\n",
    "            entity = keyword.split(\"individual: \")[-1].strip()\n",
    "            entities.append(entity)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def find_entities_in_text(\n",
    "    text: str, entities: list[str]\n",
    ") -> dict[str, list[tuple[int, int]]]:\n",
    "    \"\"\"Finds all occurrences of a list of entity strings in a text.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search within.\n",
    "        entities: A list of string entities to find.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are the found entities and values are lists\n",
    "        of (start, end) character offset tuples for each occurrence.\n",
    "    \"\"\"\n",
    "    found_entities = {}\n",
    "    for entity in entities:\n",
    "        if locations := find_exact_matches(text, entity):\n",
    "            found_entities[entity] = locations\n",
    "    return found_entities\n",
    "\n",
    "\n",
    "def find_exact_matches(text: str, search_string: str) -> list[tuple[int, int]]:\n",
    "    \"\"\"Finds occurrences of the exact string using word boundaries.\n",
    "\n",
    "    This ensures that substrings like \"34 Normae\" are not matched when searching\n",
    "    for \"Normae\".\n",
    "\n",
    "    Args:\n",
    "        text: The string to search within.\n",
    "        search_string: The exact string to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the (start, end) index\n",
    "        of a match. The end index is inclusive.\n",
    "    \"\"\"\n",
    "    if not search_string:\n",
    "        return []\n",
    "\n",
    "    # Use word boundaries to ensure whole-word matching.\n",
    "    pattern = re.compile(r'\\b' + re.escape(search_string) + r'\\b')\n",
    "\n",
    "    occurrences = []\n",
    "    for match in pattern.finditer(text):\n",
    "        start_index = match.start()\n",
    "        end_index = match.end() - 1  # End index is inclusive\n",
    "        occurrences.append((start_index, end_index))\n",
    "    return occurrences\n",
    "\n",
    "\n",
    "def search_title_for_entities(\n",
    "    doc: dict, entities: list[str]\n",
    ") -> tuple[str, dict[str, list[tuple[int, int]]]]:\n",
    "    \"\"\"Searches the document title for specified entities.\n",
    "\n",
    "    Args:\n",
    "        doc: A dictionary representing a single document from the ADS API.\n",
    "        entities: A list of string entities to find.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the document title text and a dictionary of\n",
    "        found entities with their character offsets.\n",
    "    \"\"\"\n",
    "    text = \" \".join(doc.get(\"title\", []))\n",
    "    found_entities = find_entities_in_text(text, entities)\n",
    "    return text, found_entities\n",
    "\n",
    "\n",
    "def search_abstract_for_entities(\n",
    "    doc: dict, entities: list[str]\n",
    ") -> tuple[str, dict[str, list[tuple[int, int]]]]:\n",
    "    \"\"\"Searches the document abstract for specified entities.\n",
    "\n",
    "    Args:\n",
    "        doc: A dictionary representing a single document from the ADS API.\n",
    "        entities: A list of string entities to find.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the document abstract text and a dictionary of\n",
    "        found entities with their character offsets.\n",
    "    \"\"\"\n",
    "    text = doc.get(\"abstract\", \"\")\n",
    "    found_entities = find_entities_in_text(text, entities)\n",
    "    return text, found_entities\n",
    "\n",
    "def build_data_model(doc):\n",
    "    \"\"\"\n",
    "    Builds the data structure for training a NER model.\n",
    "    \"\"\"\n",
    "    entities = extract_astronomical_keyword_entities(doc)\n",
    "    if not entities:\n",
    "        return None\n",
    "    \n",
    "    data_model = {\n",
    "        \"doc_id\": doc.get(\"id\"),\n",
    "        \"title\": doc.get(\"title\", \"\"),\n",
    "        \"abstract\": doc.get(\"abstract\", \"\"),\n",
    "        \"keywords\": doc.get(\"keyword\", []),\n",
    "        \"objects\": entities,\n",
    "    }\n",
    "    \n",
    "    title, title_entities = search_title_for_entities(doc, entities)\n",
    "    abstract, abstract_entities = search_abstract_for_entities(doc, entities)\n",
    "\n",
    "    data_model[\"title_entities\"] = title_entities\n",
    "    data_model[\"abstract_entities\"] = abstract_entities\n",
    "\n",
    "    spacy_ner_data = []\n",
    "    if title_entities:\n",
    "        spacy_ner_data.extend(build_spacy_ner_data(title, title_entities))\n",
    "    if abstract_entities:\n",
    "        spacy_ner_data.extend(build_spacy_ner_data(abstract, abstract_entities))\n",
    "    data_model[\"spacy_ner_data\"] = spacy_ner_data\n",
    "    \n",
    "    return data_model\n",
    "\n",
    "def build_spacy_ner_data(text: str, entities: dict[str, list[tuple[int]]]) -> list[dict]:\n",
    "    \"\"\"Builds the data structure for training a spaCy NER model.\n",
    "\n",
    "    This function converts a text and a dictionary of entity locations into the\n",
    "    specific format required for a single spaCy training example. The format is\n",
    "    a list containing the text and a dictionary with an \"entities\" key.\n",
    "    The value of \"entities\" is a list of tuples, where each tuple represents\n",
    "    a single entity with its start offset, end offset (inclusive), and label.\n",
    "\n",
    "    Args:\n",
    "        text: The source text containing the entities.\n",
    "        entities: A dictionary where keys are entity names and values are lists\n",
    "                  of (start, end) character offsets for each occurrence.\n",
    "\n",
    "    Returns:\n",
    "        A list containing the text and an entity dictionary, formatted for\n",
    "        spaCy training. For example:\n",
    "        ['Some text about SN 2023ixf.', {'entities': [(16, 25, 'ASTRO_OBJ')]}]\n",
    "    \"\"\"\n",
    "    ner_data = [text,\n",
    "                {\n",
    "                    \"entities\": [(*loc, \"ASTRO_OBJ\") for _, locs in entities.items() for loc in locs]\n",
    "                },\n",
    "                ]\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_abstracts(data_dir: Path) -> List[Dict[str, Any]]:\n",
    "     \"\"\"\n",
    "     Reads ADS abstracts from JSON files in the given directory.\n",
    " \n",
    "     Args:\n",
    "         data_dir: Path to the directory containing the JSON files.\n",
    " \n",
    "     Returns:\n",
    "         A list of dictionaries, where each dictionary represents an abstract.\n",
    "     \"\"\"\n",
    "     abstract_files = list(data_dir.glob(\"*.json\"))\n",
    "     all_abstracts: List[Dict[str, Any]] = []\n",
    "     for file in abstract_files:\n",
    "         with open(file, \"r\") as f:\n",
    "             abstract_batch = json.load(f).get(\"response\")\n",
    "             docs = abstract_batch.get(\"docs\", [])\n",
    "             all_abstracts.extend(docs)\n",
    "     return all_abstracts\n",
    " \n",
    " \n",
    "def build_data_model(doc: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "     \"\"\"\n",
    "     Builds a data model from a single document (abstract).  This is a placeholder;\n",
    "     replace with your actual data model building logic.\n",
    " \n",
    "     Args:\n",
    "         doc: A dictionary representing a single abstract document.\n",
    " \n",
    "     Returns:\n",
    "         An optional dictionary representing the data model, or None if the model\n",
    "         cannot be built from the document.\n",
    "     \"\"\"\n",
    "     # Replace this with your actual data model building logic\n",
    "     # This is just a placeholder\n",
    "     try:\n",
    "         title = doc.get(\"title\", \"N/A\")\n",
    "         abstract = doc.get(\"abstract\", \"N/A\")\n",
    "         return {\"title\": title, \"abstract\": abstract}  # Example data model\n",
    "     except:\n",
    "         return None\n",
    " \n",
    " \n",
    "def process_abstracts(abstracts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "     \"\"\"\n",
    "     Processes a list of abstracts to build data models.\n",
    " \n",
    "     Args:\n",
    "         abstracts: A list of dictionaries, where each dictionary represents an abstract.\n",
    " \n",
    "     Returns:\n",
    "         A list of data models (dictionaries) built from the abstracts.\n",
    "     \"\"\"\n",
    "     spacy_ner_data: List[Dict[str, Any]] = []\n",
    "     for doc in abstracts:\n",
    "         data_model = build_data_model(doc)\n",
    "         if data_model:\n",
    "             spacy_ner_data.append(data_model)\n",
    "     return spacy_ner_data\n",
    " \n",
    " \n",
    "def write_data(data: List[Dict[str, Any]], output_path: Path) -> None:\n",
    "     \"\"\"\n",
    "     Writes the processed data to a JSON file.\n",
    " \n",
    "     Args:\n",
    "         data: A list of dictionaries to write to the file.\n",
    "         output_path: The path to the output JSON file.\n",
    "     \"\"\"\n",
    "     output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "     with open(output_path, \"w\") as f:\n",
    "         json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "     print(f\"Saved spaCy NER data to {output_path}\")\n",
    " \n",
    " \n",
    "def main(data_root: Path, output_path: Path) -> None:\n",
    "     \"\"\"\n",
    "     Main function to execute the data processing pipeline.\n",
    " \n",
    "     Args:\n",
    "         data_root: The root directory containing the ADS abstracts.\n",
    "         output_path: The path to save the processed data.\n",
    "     \"\"\"\n",
    "     ads_data = data_root / \"ads_abstracts\"\n",
    "     abstracts = read_abstracts(ads_data)\n",
    "     processed_data = process_abstracts(abstracts)\n",
    "     write_data(processed_data, output_path)\n",
    " \n",
    " \n",
    " if __name__ == \"__main__\":\n",
    "     DATA_ROOT = Path(\"../data\")\n",
    "     OUTPATH = DATA_ROOT / \"ner_data\" / \"spacy_ner_data.json\"\n",
    "     main(DATA_ROOT, OUTPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6359314",
   "metadata": {},
   "source": [
    "## Example implementation\n",
    "\n",
    "Testing the functions before we build a more generic script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e8f46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data inputs\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "ADS_DATA = DATA_ROOT / \"ads_abstracts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d03639f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ADS abstracts \n",
    "abstract_files = list(ADS_DATA.glob(\"*.json\"))\n",
    "spacy_ner_data = []\n",
    "\n",
    "for file in abstract_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        abstract_batch = json.load(f).get(\"response\")\n",
    "        docs = abstract_batch.get(\"docs\", [])\n",
    "    for doc in docs:\n",
    "        data_model = build_data_model(doc)\n",
    "        if data_model:\n",
    "            spacy_ner_data.append(data_model)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d0cb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "002c0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved spaCy NER data to ../data/ner_data/spacy_ner_data.json\n"
     ]
    }
   ],
   "source": [
    "#Output data to disk\n",
    "OUTPATH = DATA_ROOT / \"ner_data\" / \"spacy_ner_data.json\"\n",
    "\n",
    "with open(OUTPATH, \"w\") as f:\n",
    "    json.dump(spacy_ner_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved spaCy NER data to {OUTPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5afb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astronomical-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
