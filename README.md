# ğŸš€ Astronomical Source NER Pipeline

## ğŸ§  Project Overview

This project develops a custom **Named Entity Recognition (NER)** model that extracts references to **celestial sources** (e.g., quasars, black holes, gamma-ray bursts) from scientific abstracts and real-time alerts. 

The model is trained using astronomy abstracts from **NASA ADS** and then deployed to identify source objects in the **Astronomer's Telegram (ATel)** RSS feed. It is designed with **full MLOps capabilities**, including experiment tracking, workflow orchestration, model deployment, monitoring, and reproducibility.

---

## ğŸ¯ Problem Description

In astrophysics, the discovery and study of celestial sources are documented in scientific literature and real-time alert networks like ATel. However, identifying and indexing source names from this unstructured data is challenging due to inconsistent formatting, varied nomenclature, and volume.

This project addresses that by:
- Creating a labeled dataset of astronomical sources from NASA ADS abstracts
- Fine-tuning a `spaCy` NER model to recognize source names
- Applying the model to ATel RSS feed entries to extract entities
- Providing a **Streamlit web app** for real-time interaction

---

## â˜ï¸ Cloud Architecture

We use **AWS** as the cloud platform and **Terraform** for Infrastructure as Code (IaC) to provision and manage resources.

### Cloud Components

| Component         | Technology                      |
|-------------------|----------------------------------|
| Cloud Provider    | AWS (S3, Lambda, CloudWatch)     |
| IaC               | Terraform                        |
| Data Storage      | S3                               |
| Workflow Scheduler| Prefect Cloud (Free Tier)        |
| Model Hosting     | AWS Lambda                       |
| Monitoring        | Grafana, Evidently, CloudWatch   |
| CI/CD             | GitHub Actions                   |
| Web App           | Streamlit (locally or on ECS)    |

âœ… This setup meets the 4-point cloud criterion: fully cloud-hosted with IaC.

---

## ğŸ§ª Experiment Tracking and Model Registry

All experiments are tracked with **MLflow**, which logs:
- spaCy model versions
- Dataset versions
- Hyperparameters and metrics (precision, recall, F1)

A **model registry** tracks models in staging and production.

âœ… This setup meets the 4-point experiment tracking criterion: tracking + registry used.

---

## ğŸ” Workflow Orchestration

Workflows are built and scheduled with **Prefect Cloud**:

### Flows:
1. `build_dataset`: Parse and label NASA abstracts
2. `train_model`: Train spaCy NER model
3. `evaluate_model`: Run evaluation and log metrics
4. `inference_atel`: Poll ATel RSS feed and extract sources
5. `monitor`: Run drift reports with Evidently

âœ… Fully deployed with remote Prefect agent and schedules.

---

## ğŸš€ Model Deployment

- **Batch inference** is performed via **AWS Lambda** (low-cost and scalable).
- spaCy model is compiled to optimize Lambda performance.
- Outputs written to **S3** and logged to **CloudWatch**.

### Streamlit Demo
A **Streamlit web app** enables users to:
- Paste in text (e.g., ATel message)
- View extracted source names
- Can run locally or be deployed to AWS ECS

âœ… Meets 4-point deployment criterion: containerized + cloud-executable.

---

## ğŸ“ˆ Monitoring

Monitoring is conducted via:
- **Evidently** for drift detection and label distribution checks
- **Grafana dashboards** via CloudWatch metrics (cold starts, errors, latency)

### Conditional Flow
If metrics degrade (e.g., F1 drop, drift detected):
- Prefect triggers a **retraining flow**
- Slack/email alerts notify maintainers

âœ… Meets 4-point monitoring criterion: alerts and conditional logic included.

---

## ğŸ§¹ Reproducibility

- Project uses **`uv`** for fast, modern dependency and environment management
- Code and data are versioned
- All flows are reproducible via Prefect
- Infrastructure managed by Terraform

âœ… Meets 4-point reproducibility criterion.

---

## âœ… MLOps Best Practices

| Practice                  | Status             |
|---------------------------|--------------------|
| Unit tests                | âœ… `pytest` in `tests/unit/` |
| Integration tests         | âœ… `pytest` in `tests/integration/` |
| Linter/Formatter          | âœ… `black`, `ruff` |
| Makefile                  | âœ… Automation for test/lint/train |
| Pre-commit hooks          | âœ… `pre-commit` used |
| CI/CD                     | âœ… GitHub Actions  |

âœ… Full 7/7 points for MLOps best practices.

---

## ğŸ—‚ï¸ Project Structure

```bash
astronomical-ner/
â”œâ”€â”€ data/                      # Raw + processed ADS abstracts
â”œâ”€â”€ notebooks/                 # EDA and dataset building
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion/        # Download from NASA ADS
â”‚   â”œâ”€â”€ labeling/              # Create spaCy-compatible annotations
â”‚   â”œâ”€â”€ training/              # Model training logic
â”‚   â”œâ”€â”€ inference/             # ATel RSS + entity extraction
â”‚   â”œâ”€â”€ monitoring/            # Evidently integration
â”‚   â”œâ”€â”€ app/                   # Streamlit demo app
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                  # Unit tests
â”‚   â”œâ”€â”€ integration/           # End-to-end tests
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ lambda/                # Lightweight Lambda inference logic
â”‚   â”œâ”€â”€ docker/                # Container setup for app and model
â”œâ”€â”€ terraform/                # IaC: S3, Lambda, IAM, CloudWatch
â”œâ”€â”€ .github/workflows/        # CI/CD pipelines
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ README.md

# ğŸ§ª How to Run (Dev Mode) â€” Using uv
## Install uv (if not installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

## Install dependencies
uv pip install -r requirements.txt

## Lint + format code
uv pip install black ruff
black .
ruff check . --fix

## Run all tests
uv pip install pytest
pytest

## Run specific test sets
pytest tests/unit/
pytest tests/integration/

## Launch Streamlit app
streamlit run src/app/app.py


